{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naitiksinhsolanki/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch._tensor\n",
    "import torchmetrics\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection \n",
    "import torch.utils.data as data \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Dataset/final_train_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m./Dataset/final_train_df.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./Dataset/final_test_df.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_x86/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Dataset/final_train_df.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./Dataset/final_train_df.csv')\n",
    "test_df = pd.read_csv('./Dataset/final_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label = train_df[['HasDetections']]\n",
    "train_df_feature = train_df.drop(axis='columns', labels='HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1622326, 137)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([train_df_feature, test_df])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prep.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_df_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_feature, test_df = X[:train_size], X[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1265466, 137)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X[:train_size], X[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tr, X_train_te, y_train_tr, y_train_te = model_selection.train_test_split(train_df_feature,\n",
    "                                                                                  train_df_label,\n",
    "                                                                                  test_size=0.2,\n",
    "                                                                                  random_state=42,\n",
    "                                                                                  stratify=train_df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2048\n",
    "\n",
    "# train_data = Data(X_train_tr.to_numpy(), y_train_tr.to_numpy().reshape(-1,))\n",
    "# train_dataloader = data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# batch_size = 1\n",
    "\n",
    "# test_data = Data(X_train_te.to_numpy(), y_train_te.to_numpy().reshape(-1,))\n",
    "# test_dataloader = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8192\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_data \u001b[39m=\u001b[39m Data(X_train_tr, y_train_tr\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m      4\u001b[0m train_dataloader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mtrain_data, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 8192\n",
    "\n",
    "train_data = Data(X_train_tr, y_train_tr.to_numpy().reshape(-1,))\n",
    "train_dataloader = data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "test_data = Data(X_train_te, y_train_te.to_numpy().reshape(-1,))\n",
    "test_dataloader = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork(nn.Module):\n",
    "    def __init__(self, hyperparameters):\n",
    "        super(neuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hyperparameters['input-dimension'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-4']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-4'], hyperparameters['hidden-dimension-4']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-4'], hyperparameters['hidden-dimension-4']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-4'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-3']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-1']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-3'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-2']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-2']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-2'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['hidden-dimension-1']),\n",
    "            nn.ELU(alpha=hyperparameters['elu-alpha']),\n",
    "            nn.Dropout(hyperparameters['dropout-3']),\n",
    "            \n",
    "            nn.Linear(hyperparameters['hidden-dimension-1'], hyperparameters['output-dimension']),\n",
    "        )\n",
    "\n",
    "      \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'input-dimension': 137,\n",
    "    'hidden-dimension-1': 2048,\n",
    "    'hidden-dimension-2': 1024,\n",
    "    'hidden-dimension-3': 512,\n",
    "    'hidden-dimension-4': 256,\n",
    "    'output-dimension': 10,\n",
    "    'dropout-1': 0.3,\n",
    "    'dropout-2': 0.35,\n",
    "    'dropout-3': 0.45,\n",
    "    'elu-alpha': 1.5,\n",
    "}\n",
    "\n",
    "model = neuralNetwork(hyperparameters=hyperparameters)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optmiser options\n",
    "- Adam()\n",
    "- AdamW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000008\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                            #  weight_decay=0.00001\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   | Loss 0.709845\n",
      "Epoch 25  | Loss 0.676472\n",
      "Epoch 50  | Loss 0.670556\n",
      "Epoch 75  | Loss 0.664058\n",
      "Epoch 100 | Loss 0.661448\n",
      "Epoch 125 | Loss 0.665875\n",
      "Epoch 150 | Loss 0.66345\n",
      "Epoch 175 | Loss 0.662519\n",
      "Epoch 200 | Loss 0.655094\n",
      "Epoch 225 | Loss 0.659403\n",
      "Epoch 250 | Loss 0.667426\n",
      "Epoch 275 | Loss 0.65363\n",
      "Epoch 300 | Loss 0.655268\n",
      "Epoch 325 | Loss 0.65657\n",
      "Epoch 350 | Loss 0.65643\n",
      "Epoch 375 | Loss 0.65657\n",
      "Epoch 400 | Loss 0.655731\n",
      "Epoch 425 | Loss 0.645878\n",
      "Epoch 450 | Loss 0.650708\n",
      "Epoch 475 | Loss 0.642491\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        (X, y) = (X.to(device), y.to(device))\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        loss_values.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%25 == 0:\n",
    "        print(\"Epoch {:<3n} | Loss {:>5n}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500 test loss: 0.650 test accuracy: 0.622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testLoss = 0\n",
    "testAcc = 0\n",
    "samples = 0\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_trut = []\n",
    "\n",
    "# initialize a no-gradient context\n",
    "with torch.no_grad():\n",
    "    # loop over the current batch of test data\n",
    "    for X, y in test_dataloader:\n",
    "        # flash the data to the current device\n",
    "        (X, y) = (X.to(device), y.to(device))\n",
    "        # run data through our model and calculate loss\n",
    "        predictions = model(X)\n",
    "        loss = loss_fn(predictions, y.long())\n",
    "        # update test loss, accuracy, and the number of\n",
    "        # samples visited\n",
    "        testLoss += loss.item() * y.size(0)\n",
    "        testAcc += (predictions.max(1)[1] == y).sum().item()\n",
    "        y_pred.append(predictions.max(1)[1].to('cpu').detach().numpy())\n",
    "        y_trut.append(y.to('cpu').detach().numpy())\n",
    "        samples += y.size(0)\n",
    "    # display model progress on the current test batch\n",
    "    testTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "    print(testTemplate.format(epoch + 1, (testLoss / samples),\n",
    "        (testAcc / samples)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rep = classification_report(y_trut, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.54      0.59    124512\n",
      "         1.0       0.61      0.70      0.65    128582\n",
      "\n",
      "    accuracy                           0.62    253094\n",
      "   macro avg       0.62      0.62      0.62    253094\n",
      "weighted avg       0.62      0.62      0.62    253094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(c_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215121654405082"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_trut, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STUDENT_ROLLNO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pooja/pooja/unil/videos/1629557170507/Test/NeuralNetwork.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130332e3135362e31392e323434222c2275736572223a22706f6f6a61222c22706f7274223a33333030367d/home/pooja/pooja/unil/videos/1629557170507/Test/NeuralNetwork.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@PROTECTED_2\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130332e3135362e31392e323434222c2275736572223a22706f6f6a61222c22706f7274223a33333030367d/home/pooja/pooja/unil/videos/1629557170507/Test/NeuralNetwork.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(STUDENT_ROLLNO,STUDENT_NAME))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STUDENT_ROLLNO' is not defined"
     ]
    }
   ],
   "source": [
    "#@PROTECTED_2\n",
    "np.save(\"{}__{}\".format(STUDENT_ROLLNO,STUDENT_NAME))\n",
    "#@PROTECTED_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_x86",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a043d14b8cd97c277bd69f21a70d9b0f47eb4c30bc0adfc62a3b121f904840ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
